# ğŸ“š ExplicaciÃ³n Completa del CÃ³digo - Detector de Smishing

## ğŸ¯ Resumen Ejecutivo

Este modelo detecta mensajes SMS fraudulentos (smishing) con **96% de accuracy** combinando:
- **BERT** (BETO) para comprensiÃ³n semÃ¡ntica del texto
- **23 caracterÃ­sticas numÃ©ricas** para patrones especÃ­ficos de fraude
- **Arquitectura dual** que fusiona ambas fuentes de informaciÃ³n

---

## ğŸ“Š Resultados Finales

```
âœ… Accuracy: 96%
âœ… Precision: 96%
âœ… Recall: 97.16%
âœ… Especificidad: 95.74%
âœ… AUC-ROC: 99.18%
âœ… Falsos Positivos: 4.3% (6/141)
âœ… Falsos Negativos: 2.8% (4/141)
```

---

## ğŸ—ï¸ Arquitectura General

```
Mensaje SMS + Remitente
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ExtracciÃ³n de      â”‚
    â”‚ CaracterÃ­sticas    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                 â†“
BERT (768)      NumÃ©ricas (23)
    â†“                 â†“
Dense(256)       Dense(128)
    â†“                 â†“
Dense(128)       Dense(64)
    â†“                 â†“
    â””â”€â”€â”€â”€â”€â”€ Concatenate â”€â”€â”˜
              â†“
          Dense(128)
              â†“
          Dense(64)
              â†“
       Dense(1, sigmoid)
              â†“
    Probabilidad [0-1]
```

---

## ğŸ”§ ConfiguraciÃ³n Optimizada

### ParÃ¡metros Globales

```python
MAX_LENGTH = 128           # Tokens BERT (reducido de 512 para mejor generalizaciÃ³n)
BATCH_SIZE = 32            # TamaÃ±o de lote (aumentado para estabilidad)
EPOCHS = 15                # Ã‰pocas mÃ¡ximas (con early stopping)
LEARNING_RATE = 2e-4       # Tasa de aprendizaje (aumentada para convergencia)
SEED = 42                  # Semilla para reproducibilidad
FINE_TUNE_BERT = False     # Fine-tuning desactivado (no necesario)
```

### Reproducibilidad Completa

```python
# Semillas globales
os.environ['PYTHONHASHSEED'] = str(SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
tf.config.experimental.enable_op_determinism()
```

**Resultado**: Entrenamientos idÃ©nticos cada vez (variaciÃ³n < 0.01%)

---

## ğŸ“ Funciones Principales

### 1. `cargar_bert()` - Carga Lazy de BERT

```python
def cargar_bert():
    global tokenizer, bert_model
    if tokenizer is None or bert_model is None:
        tokenizer = BertTokenizerFast.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
        bert_model = TFBertModel.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
    return tokenizer, bert_model
```

**Â¿Por quÃ© BETO?**
- Entrenado especÃ­ficamente en espaÃ±ol
- 110M parÃ¡metros
- 768 dimensiones de embeddings
- Mejor comprensiÃ³n del contexto en espaÃ±ol que modelos multilingÃ¼es

---

### 2. `cargar_datos()` - Carga y Preprocesamiento

```python
def cargar_datos(ruta_archivo):
    # Leer CSV
    df = pd.read_csv(ruta_archivo)
    
    # Extraer mensajes fraudulentos
    df_fraude = df[df['MensajesF'].notna()].copy()
    df_fraude['mensaje'] = df_fraude['MensajesF']
    df_fraude['es_fraude'] = 1
    
    # Extraer mensajes legÃ­timos
    df_legitimo = df[df['MensajesV'].notna()].copy()
    df_legitimo['mensaje'] = df_legitimo['MensajesV']
    df_legitimo['es_fraude'] = 0
    
    # Combinar
    df_combinado = pd.concat([df_fraude, df_legitimo], ignore_index=True)
    
    return df_combinado
```

**Dataset resultante**:
- 1405 mensajes (703 fraude + 703 legÃ­timos)
- Perfectamente balanceado (50/50)

---

### 3. `extraer_caracteristicas_mejoradas()` - 23 CaracterÃ­sticas

Esta es la funciÃ³n mÃ¡s importante. Extrae caracterÃ­sticas que BERT no puede capturar directamente.

#### CaracterÃ­sticas del Mensaje (4)

```python
# 1. Longitud del mensaje
df['mensaje_longitud'] = df['mensaje'].apply(lambda x: len(str(x)))

# 2. NÃºmero de palabras
df['mensaje_palabras'] = df['mensaje'].apply(lambda x: len(str(x).split()))

# 3. Ratio de mayÃºsculas
df['mensaje_mayusculas_ratio'] = df['mensaje'].apply(
    lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)
)

# 4. Caracteres especiales
df['mensaje_caracteres_especiales'] = df['mensaje'].apply(
    lambda x: sum(1 for c in str(x) if not c.isalnum() and not c.isspace()) / max(len(str(x)), 1)
)
```

#### CaracterÃ­sticas del Remitente (7)

```python
# 5. Longitud del remitente
df['remitente_longitud'] = df['remitente'].apply(lambda x: len(str(x)))

# 6. Es numÃ©rico
df['remitente_es_numerico'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() else 0
)

# 7. Tiene letras
df['remitente_tiene_letras'] = df['remitente'].apply(
    lambda x: 1 if any(c.isalpha() for c in str(x)) else 0
)

# 8. Empieza por 3 (mÃ³viles colombianos) â­ CLAVE
df['remitente_empieza_3'] = df['remitente'].apply(
    lambda x: 1 if str(x).startswith('3') and str(x).isdigit() else 0
)

# 9. NÃºmero corto (4-6 dÃ­gitos - servicios legÃ­timos)
df['remitente_numero_corto'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() and 4 <= len(str(x)) <= 6 else 0
)

# 10. MÃ³vil estÃ¡ndar (10 dÃ­gitos con 3)
df['remitente_movil_estandar'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() and len(str(x)) == 10 and str(x).startswith('3') else 0
)

# 11. Longitud anormal (sospechoso)
def longitud_anormal(remitente):
    if not str(remitente).isdigit():
        return 0
    longitud = len(str(remitente))
    return 1 if longitud not in [4, 5, 6, 10] else 0
```

**Â¿Por quÃ© estas caracterÃ­sticas?**
- NÃºmeros cortos (4-6): CÃ³digos de servicio legÃ­timos (DiDi, Uber)
- MÃ³viles (10 dÃ­gitos con 3): Pueden ser legÃ­timos o fraude
- Longitud anormal: Muy sospechoso

#### CaracterÃ­sticas de Contenido (8)

```python
# 12. Contiene URL â­â­â­
df['contiene_url'] = df['mensaje'].apply(
    lambda x: 1 if re.search(r'http[s]?://|www\.|\.com|\.org|\.net|bit\.ly|\.co\b', str(x).lower()) else 0
)

# 13. Palabras de urgencia â­â­â­
palabras_urgencia = ['urgente', 'inmediatamente', 'ahora', 'rÃ¡pido', 'expira', 'vence', ...]
df['contiene_urgencia'] = df['mensaje'].apply(
    lambda x: 1 if any(palabra in str(x).lower() for palabra in palabras_urgencia) else 0
)

# 14. Palabras de dinero
palabras_dinero = ['$', 'pesos', 'dinero', 'gratis', 'premio', 'ganador', ...]

# 15. Palabras bancarias â­â­â­
palabras_banco = ['banco', 'bancolombia', 'davivienda', 'nequi', 'cuenta', ...]

# 16. Palabras de verificaciÃ³n â­â­â­
palabras_verificacion = ['verificar', 'confirmar', 'validar', 'actualizar', ...]

# 17. Servicios conocidos (legÃ­timos)
servicios_legitimos = ['didi', 'uber', 'rappi', 'bancolombia', ...]

# 18. Errores ortogrÃ¡ficos (comÃºn en fraudes)
palabras_error = ['isu', 'ingrese', 'confirme', 'verifique', ...]

# 19. Llamada a acciÃ³n sospechosa
llamadas_accion = ['haz clic', 'ingresa', 'entra', 'visita', ...]
```

#### CaracterÃ­sticas Combinadas (4) â­â­â­â­â­

```python
# 20. Sospecha mÃ³vil fraudulento â­â­â­â­â­
# MÃ³vil colombiano + seÃ±ales de fraude
df['sospecha_movil_fraudulento'] = (
    (df['remitente_empieza_3'] == 1) & 
    ((df['contiene_url'] == 1) | 
     (df['contiene_verificacion'] == 1) | 
     (df['tiene_errores_ortograficos'] == 1))
).astype(int)

# 21. Contiene premio
df['contiene_premio'] = df['mensaje'].apply(
    lambda x: 1 if any(palabra in str(x).lower() for palabra in ['ganaste', 'premio', 'sorteo']) else 0
)

# 22. Monto grande (>$100,000)
df['monto_grande'] = df['mensaje'].apply(
    lambda x: 1 if re.search(r'\$\s*[1-9]\d{5,}|\d{1,3}(?:[.,]\d{3}){2,}', str(x)) else 0
)

# 23. PatrÃ³n estafa premio â­â­â­â­â­
# Premio/monto grande + URL/llamada a acciÃ³n
df['patron_estafa_premio'] = (
    ((df['contiene_premio'] == 1) | (df['monto_grande'] == 1)) &
    ((df['contiene_url'] == 1) | (df['llamada_accion_sospechosa'] == 1))
).astype(int)
```

**Â¿Por quÃ© estas son las mÃ¡s importantes?**
- Capturan **patrones complejos** que BERT no ve
- Son **combinaciones lÃ³gicas** de seÃ±ales simples
- **sospecha_movil_fraudulento**: PatrÃ³n clave en Colombia
- **patron_estafa_premio**: Detecta fraudes de premios falsos

---

### 4. `extraer_caracteristicas_bert()` - Embeddings de BERT

```python
def extraer_caracteristicas_bert(textos, max_length=MAX_LENGTH):
    # Cargar BERT
    tokenizer, bert_model = cargar_bert()
    
    # Tokenizar
    tokens = tokenizer(
        textos.tolist(),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='tf'
    )
    
    # Procesar por lotes (para eficiencia)
    batch_size = 8
    all_features = []
    
    for i in range(0, len(textos), batch_size):
        batch_input_ids = tokens['input_ids'][i:i+batch_size]
        batch_attention_mask = tokens['attention_mask'][i:i+batch_size]
        
        # Obtener embeddings
        outputs = bert_model(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_mask
        )
        
        # Guardar pooled output (representaciÃ³n del [CLS] token)
        all_features.append(outputs.pooler_output.numpy())
    
    return np.vstack(all_features)
```

**Â¿QuÃ© hace BERT?**
1. **TokenizaciÃ³n**: "Ganaste $5M" â†’ [101, 2345, 678, 102, ...]
2. **Embeddings**: Cada token â†’ vector de 768 dims
3. **Contexto**: Entiende relaciones entre palabras
4. **Pooled Output**: Resumen del mensaje (768 dims)

**Tiempo**: ~0.5-1 seg por mensaje en CPU, ~0.01 seg en GPU

---

### 5. `crear_modelo_mejorado()` - Arquitectura Optimizada

```python
def crear_modelo_mejorado(num_features):
    # Inicializador determinÃ­stico
    initializer = tf.keras.initializers.GlorotUniform(seed=SEED)
    
    # ENTRADAS
    bert_input = Input(shape=(768,), name='bert_features')
    num_input = Input(shape=(num_features,), name='num_features')
    
    # RAMA BERT - RegularizaciÃ³n agresiva
    bert_branch = Dense(256, activation='relu', 
                       kernel_regularizer=l2(0.01),
                       kernel_initializer=initializer)(bert_input)
    bert_branch = BatchNormalization()(bert_branch)
    bert_branch = Dropout(0.5, seed=SEED)(bert_branch)
    bert_branch = Dense(128, activation='relu', 
                       kernel_regularizer=l2(0.01),
                       kernel_initializer=initializer)(bert_branch)
    bert_branch = BatchNormalization()(bert_branch)
    bert_branch = Dropout(0.4, seed=SEED)(bert_branch)
    
    # RAMA NUMÃ‰RICA - ConfiguraciÃ³n comprobada
    num_branch = Dense(128, activation='relu', 
                      kernel_regularizer=l2(0.01),
                      kernel_initializer=initializer)(num_input)
    num_branch = BatchNormalization()(num_branch)
    num_branch = Dropout(0.4, seed=SEED)(num_branch)
    num_branch = Dense(64, activation='relu', 
                      kernel_regularizer=l2(0.01),
                      kernel_initializer=initializer)(num_branch)
    num_branch = BatchNormalization()(num_branch)
    num_branch = Dropout(0.3, seed=SEED)(num_branch)
    
    # COMBINAR
    combined = Concatenate()([bert_branch, num_branch])
    combined = Dense(128, activation='relu', 
                    kernel_regularizer=l2(0.01),
                    kernel_initializer=initializer)(combined)
    combined = BatchNormalization()(combined)
    combined = Dropout(0.5, seed=SEED)(combined)
    combined = Dense(64, activation='relu', 
                    kernel_regularizer=l2(0.01),
                    kernel_initializer=initializer)(combined)
    combined = Dropout(0.4, seed=SEED)(combined)
    
    # SALIDA
    output = Dense(1, activation='sigmoid', 
                  kernel_initializer=initializer,
                  name='output')(combined)
    
    model = Model(inputs=[bert_input, num_input], outputs=output)
    
    # Compilar con gradient clipping
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0),
        loss='binary_crossentropy',
        metrics=['accuracy', AUC(name='auc'), Precision(), Recall()]
    )
    
    return model
```

**ParÃ¡metros totales**: ~277K (reducido de 701K original)

**Optimizaciones aplicadas**:
- âœ… **L2 = 0.01** (10x mÃ¡s fuerte que antes)
- âœ… **Dropout 0.3-0.5** (mÃ¡s agresivo)
- âœ… **Gradient clipping** (clipnorm=1.0)
- âœ… **BatchNormalization** (estabiliza entrenamiento)
- âœ… **Inicializadores con semilla** (reproducibilidad)

---

### 6. `entrenar_modelo_balanceado()` - Entrenamiento Optimizado

```python
def entrenar_modelo_balanceado(model, X_train, y_train, X_val, y_val):
    # Calcular pesos de clase
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
    
    # Callbacks optimizados
    callbacks = [
        EarlyStopping(
            monitor='val_auc',
            patience=5,
            restore_best_weights=True,
            mode='max'
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.3,
            patience=2,
            min_lr=1e-7
        ),
        ModelCheckpoint(
            'best_model_temp.keras',
            monitor='val_auc',
            save_best_only=True,
            mode='max'
        )
    ]
    
    # Entrenar
    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1,
        shuffle=True  # â­ Shuffle en cada Ã©poca
    )
    
    return history
```

**Callbacks**:
- **EarlyStopping**: Para si no mejora en 5 Ã©pocas
- **ReduceLROnPlateau**: Reduce LR si se estanca
- **ModelCheckpoint**: Guarda mejor modelo automÃ¡ticamente

---

### 7. `encontrar_umbral_optimo()` - OptimizaciÃ³n del Umbral

```python
def encontrar_umbral_optimo(model, X_val, y_val):
    y_pred_proba = model.predict(X_val)
    
    thresholds = np.arange(0.1, 0.9, 0.01)
    f1_scores = []
    
    for threshold in thresholds:
        y_pred = (y_pred_proba >= threshold).astype(int)
        f1 = f1_score(y_val, y_pred)
        f1_scores.append(f1)
    
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    return optimal_threshold
```

**Â¿Por quÃ© no usar 0.5?**
- Maximiza F1-score (balance precision/recall)
- El modelo puede estar sesgado
- Umbral Ã³ptimo tÃ­pico: ~0.30-0.40

---

## ğŸ¯ Flujo de EjecuciÃ³n Completo

```
1. CARGA DE DATOS (5-10 seg)
   â”œâ”€ Leer CSV
   â”œâ”€ Extraer fraudes y legÃ­timos
   â””â”€ Combinar â†’ 1405 mensajes

2. EXTRACCIÃ“N DE CARACTERÃSTICAS (10-20 seg)
   â”œâ”€ 23 caracterÃ­sticas numÃ©ricas
   â””â”€ Matriz (1405, 23)

3. DIVISIÃ“N DE DATOS (1-2 seg)
   â”œâ”€ Train: 899 (64%)
   â”œâ”€ Val: 225 (16%)
   â””â”€ Test: 282 (20%)

4. BERT (5-10 min con GPU, 30-60 min con CPU)
   â”œâ”€ Cargar BETO
   â”œâ”€ Tokenizar textos
   â”œâ”€ Extraer embeddings (768 dims)
   â””â”€ Matrices: (899,768), (225,768), (282,768)

5. CREAR MODELO (5-10 seg)
   â”œâ”€ Definir arquitectura
   â”œâ”€ Compilar
   â””â”€ ~277K parÃ¡metros

6. ENTRENAR (7-8 min con GPU, 60-90 min con CPU)
   â”œâ”€ 15 Ã©pocas mÃ¡ximas
   â”œâ”€ Early stopping (tÃ­picamente para en Ã©poca 10-12)
   â”œâ”€ Balanceo de clases
   â””â”€ Guardar mejor modelo

7. OPTIMIZAR UMBRAL (1-2 min)
   â”œâ”€ Probar umbrales 0.1-0.9
   â”œâ”€ Calcular F1 para cada uno
   â””â”€ Umbral Ã³ptimo: ~0.30-0.40

8. EVALUAR (2-5 min)
   â”œâ”€ Predicciones en test
   â”œâ”€ Calcular mÃ©tricas
   â”œâ”€ Generar 7 grÃ¡ficas
   â””â”€ Mostrar resultados

9. GUARDAR (5-10 seg)
   â”œâ”€ modelo_detector_smishing_mejorado.keras
   â”œâ”€ umbral_optimo.npy
   â””â”€ 7 grÃ¡ficas PNG
```

**Tiempo total con GPU**: ~10-15 minutos
**Tiempo total con CPU**: ~90-120 minutos

---

## ğŸ“Š GrÃ¡ficas Generadas

1. **Curvas de Entrenamiento**: Loss, Accuracy, AUC, Precision/Recall por Ã©poca
2. **Matriz de ConfusiÃ³n**: Absoluta y normalizada
3. **Curva ROC**: TPR vs FPR (AUC = 0.99)
4. **Curva Precision-Recall**: Precision vs Recall
5. **MÃ©tricas por Clase**: Barras comparativas
6. **DistribuciÃ³n de Probabilidades**: Histograma de predicciones
7. **Resumen de MÃ©tricas**: Tabla visual con todas las mÃ©tricas

---

## ğŸ” Ejemplo Completo de PredicciÃ³n

### Entrada:
```
Mensaje: "Ganaste un premio de $5.000.000! Haz clic aquÃ­: bit.ly/premio123"
Remitente: "3209876543"
```

### CaracterÃ­sticas ExtraÃ­das:

**NumÃ©ricas (23)**:
```
mensaje_longitud: 67
mensaje_palabras: 9
remitente_empieza_3: 1          â­
remitente_movil_estandar: 1     â­
contiene_url: 1                 â­â­â­
contiene_dinero: 1
sospecha_movil_fraudulento: 1   â­â­â­â­â­
contiene_premio: 1              â­
monto_grande: 1                 â­
llamada_accion_sospechosa: 1    â­
patron_estafa_premio: 1         â­â­â­â­â­
... (resto en 0)
```

**BERT (768)**:
```
[0.234, -0.567, 0.891, ..., 0.123]  # Embedding semÃ¡ntico
```

### Procesamiento:

```
BERT (768) â†’ Dense(256) â†’ Dense(128) â”€â”
                                      â”œâ”€â†’ Concatenate â†’ Dense(128) â†’ Dense(64) â†’ Sigmoid
Nums (23)  â†’ Dense(128) â†’ Dense(64) â”€â”€â”˜

Salida: 0.8458 (84.58% probabilidad de fraude)
```

### DecisiÃ³n:

```
Umbral Ã³ptimo: 0.3025
0.8458 > 0.3025 â†’ ğŸš¨ FRAUDULENTO

Factores de riesgo detectados:
  - remitente_empieza_3
  - remitente_movil_estandar
  - contiene_dinero
  - contiene_verificacion
  - sospecha_movil_fraudulento â­â­â­
  - contiene_premio
  - monto_grande
  - llamada_accion_sospechosa
  - patron_estafa_premio â­â­â­
```

---

## ğŸš€ Optimizaciones Aplicadas

### 1. ConfiguraciÃ³n Global
- MAX_LENGTH: 512 â†’ 128 (mejor generalizaciÃ³n)
- BATCH_SIZE: 16 â†’ 32 (mÃ¡s estabilidad)
- EPOCHS: 3 â†’ 15 (con early stopping)
- LEARNING_RATE: 1e-5 â†’ 2e-4 (mejor convergencia)

### 2. Arquitectura
- ParÃ¡metros: 701K â†’ 277K (60% reducciÃ³n)
- Dropout: 0.2-0.4 â†’ 0.3-0.5 (mÃ¡s agresivo)
- L2: 0.001 â†’ 0.01 (10x mÃ¡s fuerte)
- Gradient clipping: Activado (clipnorm=1.0)

### 3. Callbacks
- EarlyStopping patience: 7 â†’ 5 (mÃ¡s agresivo)
- ReduceLROnPlateau factor: 0.5 â†’ 0.3 (reduce mÃ¡s)
- ModelCheckpoint: Agregado (guarda mejor modelo)

### 4. Reproducibilidad
- Semillas fijas en todos los componentes
- Operaciones determinÃ­sticas en TensorFlow
- Inicializadores con semilla
- Dropout con semilla

### 5. Entrenamiento
- Shuffle activado en cada Ã©poca
- Balanceo de clases
- Monitoreo de AUC (mejor que accuracy)

---

## â“ Preguntas Frecuentes

### Â¿Por quÃ© es lento en CPU?
- BERT tiene 110M parÃ¡metros
- Procesa cada mensaje individualmente
- GPU es 10-20x mÃ¡s rÃ¡pida

### Â¿Puedo usar solo caracterÃ­sticas sin BERT?
- SÃ­, pero perderÃ­as ~10-15% accuracy
- BERT captura contexto que caracterÃ­sticas no pueden

### Â¿Por quÃ© 23 caracterÃ­sticas?
- Balance entre informaciÃ³n y complejidad
- MÃ¡s caracterÃ­sticas â†’ mÃ¡s overfitting
- Estas 23 son las mÃ¡s discriminativas

### Â¿CÃ³mo sÃ© si funciona bien?
- Accuracy > 90% âœ…
- Recall > 95% âœ… (lo mÃ¡s importante)
- F1-Score > 0.90 âœ…
- AUC > 0.95 âœ…

### Â¿El modelo es reproducible?
- SÃ­, 100% reproducible con las semillas fijas
- Resultados idÃ©nticos en cada entrenamiento
- VariaciÃ³n < 0.01%

---

## ğŸ“š ConclusiÃ³n

El modelo combina:
- âœ… **BERT**: ComprensiÃ³n profunda del texto en espaÃ±ol
- âœ… **23 caracterÃ­sticas**: Patrones especÃ­ficos de smishing
- âœ… **Arquitectura dual**: Aprovecha ambas fuentes
- âœ… **RegularizaciÃ³n agresiva**: Previene overfitting
- âœ… **Umbral optimizado**: Maximiza F1-score
- âœ… **Reproducibilidad**: Resultados consistentes

**Resultado**: Detector robusto y preciso de smishing en espaÃ±ol con **96% accuracy**.

---

**Ãšltima actualizaciÃ³n**: Diciembre 2024
