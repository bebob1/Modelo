# üìö Explicaci√≥n Completa del C√≥digo - Detector de Smishing

## √çndice
1. [Estructura General](#estructura-general)
2. [Imports y Configuraci√≥n](#imports-y-configuraci√≥n)
3. [Funciones Principales](#funciones-principales)
4. [Flujo de Ejecuci√≥n](#flujo-de-ejecuci√≥n)
5. [Caracter√≠sticas Extra√≠das](#caracter√≠sticas-extra√≠das)
6. [Arquitectura del Modelo](#arquitectura-del-modelo)

---

## Estructura General

El c√≥digo est√° organizado en **m√≥dulos funcionales**:

```
modelo2.py
‚îú‚îÄ‚îÄ Imports y Configuraci√≥n (l√≠neas 1-56)
‚îú‚îÄ‚îÄ Carga de Datos (l√≠neas 57-120)
‚îú‚îÄ‚îÄ Extracci√≥n de Caracter√≠sticas (l√≠neas 121-380)
‚îú‚îÄ‚îÄ Modelo y Entrenamiento (l√≠neas 381-700)
‚îú‚îÄ‚îÄ Evaluaci√≥n y Gr√°ficas (l√≠neas 701-1150)
‚îî‚îÄ‚îÄ Funci√≥n Principal y Ejemplos (l√≠neas 1151-1224)
```

---

## Imports y Configuraci√≥n

### Librer√≠as Principales

```python
import pandas as pd          # Manejo de datos tabulares
import numpy as np           # Operaciones num√©ricas
import tensorflow as tf      # Framework de Deep Learning
from transformers import ... # BERT para procesamiento de texto
from sklearn import ...      # M√©tricas y divisi√≥n de datos
import matplotlib/seaborn    # Visualizaciones
```

### Configuraci√≥n de BERT

```python
try:
    from tf_keras.layers import Dense, Dropout, ...
except:
    from keras.layers import Dense, Dropout, ...
```

**¬øPor qu√©?** Compatibilidad entre Keras 3 y versiones anteriores.

### Par√°metros Globales

```python
MAX_LENGTH = 128           # Longitud m√°xima de tokens BERT
BATCH_SIZE = 8             # Tama√±o de lote para entrenamiento
EPOCHS = 25                # N√∫mero de √©pocas
LEARNING_RATE = 3e-5       # Tasa de aprendizaje
SEED = 42                  # Semilla para reproducibilidad
FINE_TUNE_BERT = False     # Fine-tuning de BERT (desactivado)
```

---

## Funciones Principales

### 1. `cargar_bert()` - Carga Lazy de BERT

```python
def cargar_bert():
    global tokenizer, bert_model
    if tokenizer is None or bert_model is None:
        # Cargar tokenizador
        tokenizer = BertTokenizerFast.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
        # Cargar modelo BERT
        bert_model = TFBertModel.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
    return tokenizer, bert_model
```

**¬øQu√© hace?**
- Carga el modelo BERT solo cuando se necesita (lazy loading)
- Usa BETO (BERT en espa√±ol de la Universidad de Chile)
- Evita cargar BERT m√∫ltiples veces

**¬øPor qu√© BETO?**
- Entrenado espec√≠ficamente en espa√±ol
- Mejor comprensi√≥n del contexto en espa√±ol
- 768 dimensiones de embeddings

---

### 2. `cargar_datos(ruta_archivo)` - Carga de Datos

```python
def cargar_datos(ruta_archivo):
    # 1. Leer archivo (CSV o Excel)
    if ruta_archivo.endswith('.csv'):
        df = pd.read_csv(ruta_archivo)
    elif ruta_archivo.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(ruta_archivo, header=1, skiprows=[0])
    
    # 2. Extraer mensajes fraudulentos
    df_fraude_temp = df[df['MensajesF'].notna()].copy()
    mensajes_fraude = df_fraude_temp['MensajesF'].values
    remitentes_fraude = df_fraude_temp['Remitente'].fillna('').astype(str).values
    
    # 3. Extraer mensajes leg√≠timos
    df_legitimo_temp = df[df['MensajesV'].notna()].copy()
    mensajes_legitimos = df_legitimo_temp['MensajesV'].values
    remitentes_legitimos = df_legitimo_temp['Remitente'].fillna('').astype(str).values
    
    # 4. Combinar en un solo DataFrame
    df_combinado = pd.concat([df_fraude, df_legitimo], ignore_index=True)
    
    return df_combinado
```

**Estructura del DataFrame resultante:**
```
| mensaje                    | remitente  | es_fraude |
|----------------------------|------------|-----------|
| "Ganaste $5M..."           | 3001234567 | 1         |
| "Tu pedido DiDi..."        | DiDi       | 0         |
```

**¬øPor qu√© esta estructura?**
- Formato est√°ndar para clasificaci√≥n binaria
- F√°cil de dividir en train/test
- Compatible con scikit-learn

---

### 3. `extraer_caracteristicas_mejoradas(df)` - Ingenier√≠a de Caracter√≠sticas

Esta es **la funci√≥n m√°s importante** para la detecci√≥n. Extrae **23 caracter√≠sticas num√©ricas**:

#### Caracter√≠sticas del Mensaje (4)

```python
# 1. Longitud del mensaje
df['mensaje_longitud'] = df['mensaje'].apply(lambda x: len(str(x)))

# 2. N√∫mero de palabras
df['mensaje_palabras'] = df['mensaje'].apply(lambda x: len(str(x).split()))

# 3. Ratio de may√∫sculas
df['mensaje_mayusculas_ratio'] = df['mensaje'].apply(
    lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)
)

# 4. Caracteres especiales
df['mensaje_caracteres_especiales'] = df['mensaje'].apply(
    lambda x: sum(1 for c in str(x) if not c.isalnum() and not c.isspace()) / max(len(str(x)), 1)
)
```

**¬øPor qu√©?**
- Mensajes fraudulentos suelen ser m√°s largos
- Uso excesivo de may√∫sculas es sospechoso
- Caracteres especiales pueden indicar URLs o formateo extra√±o

#### Caracter√≠sticas del Remitente (7)

```python
# 5. Longitud del remitente
df['remitente_longitud'] = df['remitente'].apply(lambda x: len(str(x)))

# 6. Es num√©rico
df['remitente_es_numerico'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() else 0
)

# 7. Tiene letras
df['remitente_tiene_letras'] = df['remitente'].apply(
    lambda x: 1 if any(c.isalpha() for c in str(x)) else 0
)

# 8. Empieza por 3 (m√≥vil colombiano) ‚≠ê CLAVE
df['remitente_empieza_3'] = df['remitente'].apply(
    lambda x: 1 if str(x).startswith('3') and str(x).isdigit() else 0
)

# 9. N√∫mero corto (4-6 d√≠gitos)
df['remitente_numero_corto'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() and 4 <= len(str(x)) <= 6 else 0
)

# 10. M√≥vil est√°ndar (10 d√≠gitos con 3)
df['remitente_movil_estandar'] = df['remitente'].apply(
    lambda x: 1 if str(x).isdigit() and len(str(x)) == 10 and str(x).startswith('3') else 0
)

# 11. Longitud anormal
def longitud_anormal(remitente):
    if not str(remitente).isdigit():
        return 0
    longitud = len(str(remitente))
    return 1 if longitud not in [4, 5, 6, 10] else 0
```

**¬øPor qu√© estas caracter√≠sticas?**
- **N√∫meros cortos (4-6)**: C√≥digos de servicio leg√≠timos
- **M√≥viles (10 d√≠gitos con 3)**: Pueden ser leg√≠timos o fraude
- **Longitud anormal**: Muy sospechoso
- **Empieza por 3**: Clave para contexto colombiano

#### Caracter√≠sticas de Contenido (8)

```python
# 12. Contiene URL
df['contiene_url'] = df['mensaje'].apply(
    lambda x: 1 if re.search(r'http[s]?://|www\.|\.com|\.org|\.net|bit\.ly|\.co\b', str(x).lower()) else 0
)

# 13. Palabras de urgencia
palabras_urgencia = ['urgente', 'inmediatamente', 'ahora', 'r√°pido', 'expira', 'vence', ...]
df['contiene_urgencia'] = df['mensaje'].apply(
    lambda x: 1 if any(palabra in str(x).lower() for palabra in palabras_urgencia) else 0
)

# 14. Palabras de dinero
palabras_dinero = ['$', 'pesos', 'dinero', 'gratis', 'premio', 'ganador', ...]
df['contiene_dinero'] = ...

# 15. Palabras bancarias
palabras_banco = ['banco', 'bancolombia', 'davivienda', 'nequi', 'cuenta', ...]
df['contiene_banco'] = ...

# 16. Palabras de verificaci√≥n
palabras_verificacion = ['verificar', 'confirmar', 'validar', 'actualizar', ...]
df['contiene_verificacion'] = ...

# 17. Servicios conocidos
servicios_legitimos = ['didi', 'uber', 'rappi', 'bancolombia', ...]
df['menciona_servicio_conocido'] = ...

# 18. Errores ortogr√°ficos
palabras_error = ['isu', 'ingrese', 'confirme', 'verifique', ...]
df['tiene_errores_ortograficos'] = ...
```

**¬øPor qu√©?**
- URLs son muy sospechosas en SMS
- Urgencia es t√°ctica de presi√≥n
- Combinaci√≥n banco + verificaci√≥n = phishing
- Servicios conocidos pueden ser leg√≠timos

#### Caracter√≠sticas Combinadas (4) ‚≠ê‚≠ê‚≠ê

```python
# 19. Sospecha m√≥vil fraudulento ‚≠ê‚≠ê‚≠ê
df['sospecha_movil_fraudulento'] = (
    (df['remitente_empieza_3'] == 1) & 
    ((df['contiene_url'] == 1) | 
     (df['contiene_verificacion'] == 1) | 
     (df['tiene_errores_ortograficos'] == 1))
).astype(int)

# 20. Contiene premio
df['contiene_premio'] = df['mensaje'].apply(
    lambda x: 1 if any(palabra in str(x).lower() for palabra in ['ganaste', 'premio', 'sorteo']) else 0
)

# 21. Monto grande (>$100,000)
df['monto_grande'] = df['mensaje'].apply(
    lambda x: 1 if re.search(r'\$\s*[1-9]\d{5,}|\d{1,3}(?:[.,]\d{3}){2,}', str(x)) else 0
)

# 22. Llamada a la acci√≥n sospechosa
df['llamada_accion_sospechosa'] = df['mensaje'].apply(
    lambda x: 1 if any(llamada in str(x).lower() for llamada in ['haz clic', 'ingresa', ...]) else 0
)

# 23. Patr√≥n estafa premio ‚≠ê‚≠ê‚≠ê
df['patron_estafa_premio'] = (
    ((df['contiene_premio'] == 1) | (df['monto_grande'] == 1)) &
    ((df['contiene_url'] == 1) | (df['llamada_accion_sospechosa'] == 1))
).astype(int)
```

**¬øPor qu√© estas son las m√°s importantes?**
- **sospecha_movil_fraudulento**: Detecta el patr√≥n clave (m√≥vil + se√±ales de fraude)
- **patron_estafa_premio**: Detecta fraudes de premios falsos
- Son **combinaciones l√≥gicas** de otras caracter√≠sticas
- Capturan **patrones complejos** que BERT podr√≠a no ver

---

### 4. `extraer_caracteristicas_bert(textos)` - Embeddings de BERT

```python
def extraer_caracteristicas_bert(textos, max_length=MAX_LENGTH):
    # 1. Cargar BERT
    global tokenizer, bert_model
    tokenizer, bert_model = cargar_bert()
    
    # 2. Tokenizar textos
    tokens = tokenizer(
        textos.tolist(),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='tf'
    )
    
    # 3. Procesar por lotes
    batch_size = 8
    all_features = []
    
    for i in range(0, len(textos), batch_size):
        batch_input_ids = tokens['input_ids'][i:i+batch_size]
        batch_attention_mask = tokens['attention_mask'][i:i+batch_size]
        
        # 4. Obtener embeddings de BERT
        outputs = bert_model(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_mask
        )
        
        # 5. Guardar pooled output (representaci√≥n del [CLS] token)
        all_features.append(outputs.pooler_output.numpy())
    
    # 6. Concatenar todos los lotes
    return np.vstack(all_features)
```

**¬øQu√© hace BERT?**
1. **Tokenizaci√≥n**: Convierte texto a n√∫meros
   - "Ganaste $5M" ‚Üí [101, 2345, 678, 102, ...]
2. **Embeddings**: Cada token ‚Üí vector de 768 dimensiones
3. **Contexto**: Entiende relaciones entre palabras
4. **Pooled Output**: Resumen del mensaje completo (768 dims)

**¬øPor qu√© es lento?**
- Procesa cada palabra en contexto
- 12 capas de transformers
- 110M par√°metros
- En CPU: ~0.5-1 segundo por mensaje

---

### 5. `crear_modelo_mejorado(num_features)` - Arquitectura del Modelo

```python
def crear_modelo_mejorado(num_features):
    # ENTRADAS
    bert_input = Input(shape=(768,), name='bert_features')      # BERT
    num_input = Input(shape=(num_features,), name='num_features')  # 23 caracter√≠sticas
    
    # RAMA BERT (procesa embeddings de texto)
    bert_branch = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(bert_input)
    bert_branch = BatchNormalization()(bert_branch)
    bert_branch = Dropout(0.4)(bert_branch)
    bert_branch = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(bert_branch)
    bert_branch = BatchNormalization()(bert_branch)
    bert_branch = Dropout(0.3)(bert_branch)
    
    # RAMA NUM√âRICA (procesa las 23 caracter√≠sticas)
    num_branch = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(num_input)
    num_branch = BatchNormalization()(num_branch)
    num_branch = Dropout(0.3)(num_branch)
    num_branch = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(num_branch)
    num_branch = BatchNormalization()(num_branch)
    num_branch = Dropout(0.2)(num_branch)
    num_branch = Dense(64, activation='relu')(num_branch)
    num_branch = Dropout(0.2)(num_branch)
    
    # COMBINAR AMBAS RAMAS
    combined = Concatenate()([bert_branch, num_branch])
    combined = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)
    combined = BatchNormalization()(combined)
    combined = Dropout(0.4)(combined)
    combined = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined)
    combined = Dropout(0.3)(combined)
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(0.2)(combined)
    
    # SALIDA (probabilidad de fraude)
    output = Dense(1, activation='sigmoid', name='output')(combined)
    
    model = Model(inputs=[bert_input, num_input], outputs=output)
    
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', AUC(name='auc'), Precision(), Recall()]
    )
    
    return model
```

**Arquitectura Visual:**

```
BERT (768)          Caracter√≠sticas (23)
    ‚Üì                       ‚Üì
  Dense(512)            Dense(256)
    ‚Üì                       ‚Üì
BatchNorm + Dropout   BatchNorm + Dropout
    ‚Üì                       ‚Üì
  Dense(256)            Dense(128)
    ‚Üì                       ‚Üì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Concatenate ‚îÄ‚îÄ‚îò
              ‚Üì
          Dense(256)
              ‚Üì
          Dense(128)
              ‚Üì
           Dense(64)
              ‚Üì
          Dense(1, sigmoid)
              ‚Üì
        Probabilidad [0-1]
```

**¬øPor qu√© esta arquitectura?**
- **Dos ramas**: BERT captura sem√°ntica, caracter√≠sticas capturan patrones
- **BatchNormalization**: Estabiliza el entrenamiento
- **Dropout**: Previene overfitting
- **L2 Regularization**: Penaliza pesos grandes
- **Sigmoid**: Salida entre 0 (leg√≠timo) y 1 (fraude)

---

### 6. `entrenar_modelo_balanceado()` - Entrenamiento

```python
def entrenar_modelo_balanceado(model, X_train, y_train, X_val, y_val):
    # 1. Calcular pesos de clase (balanceo)
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
    
    # 2. Callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_auc',
            patience=7,
            restore_best_weights=True,
            mode='max'
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7
        )
    ]
    
    # 3. Entrenar
    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )
    
    return history
```

**¬øQu√© hace cada componente?**

- **class_weight**: Da m√°s importancia a la clase minoritaria
- **EarlyStopping**: Para si no mejora en 7 √©pocas
- **ReduceLROnPlateau**: Reduce learning rate si se estanca
- **val_auc**: M√©trica principal (mejor que accuracy para clasificaci√≥n)

---

### 7. `encontrar_umbral_optimo()` - Optimizaci√≥n del Umbral

```python
def encontrar_umbral_optimo(model, X_val, y_val):
    # 1. Obtener probabilidades
    y_pred_proba = model.predict(X_val)
    
    # 2. Probar diferentes umbrales
    thresholds = np.arange(0.1, 0.9, 0.01)
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred = (y_pred_proba >= threshold).astype(int)
        f1 = f1_score(y_val, y_pred)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return best_threshold
```

**¬øPor qu√© no usar 0.5?**
- El modelo puede estar sesgado
- Queremos maximizar F1-score
- F1 balancea precision y recall

**Ejemplo:**
```
Umbral 0.3: Recall alto, Precision baja (muchos falsos positivos)
Umbral 0.5: Balanceado
Umbral 0.7: Precision alta, Recall bajo (muchos falsos negativos)

Umbral √≥ptimo: ~0.30 (maximiza F1)
```

---

### 8. `generar_graficas_evaluacion()` - Visualizaciones

Genera **7 gr√°ficas**:

1. **Curvas de Entrenamiento**: Loss, Accuracy, AUC, Precision/Recall
2. **Matriz de Confusi√≥n**: Absoluta y normalizada
3. **Curva ROC**: TPR vs FPR
4. **Curva Precision-Recall**: Precision vs Recall
5. **M√©tricas por Clase**: Barras comparativas
6. **Distribuci√≥n de Probabilidades**: Histograma
7. **Resumen de M√©tricas**: Tabla visual

---

## Flujo de Ejecuci√≥n

```python
if __name__ == "__main__":
    ruta_archivo = "datos_sms.csv"
    modelo, umbral_optimo = principal_mejorado(ruta_archivo)
```

### Paso a Paso:

```
1. CARGA DE DATOS (5-10 seg)
   ‚îú‚îÄ Leer CSV/Excel
   ‚îú‚îÄ Extraer fraudes y leg√≠timos
   ‚îî‚îÄ Combinar en DataFrame

2. EXTRACCI√ìN DE CARACTER√çSTICAS (10-20 seg)
   ‚îú‚îÄ 23 caracter√≠sticas num√©ricas
   ‚îî‚îÄ Retorna matriz (1406, 23)

3. DIVISI√ìN DE DATOS (1-2 seg)
   ‚îú‚îÄ Train: 899 (64%)
   ‚îú‚îÄ Val: 225 (16%)
   ‚îî‚îÄ Test: 282 (20%)

4. BERT (10-30 min en CPU) ‚è∞
   ‚îú‚îÄ Cargar BETO
   ‚îú‚îÄ Tokenizar textos
   ‚îú‚îÄ Extraer embeddings (768 dims)
   ‚îî‚îÄ Retorna matrices (899,768), (225,768), (282,768)

5. CREAR MODELO (5-10 seg)
   ‚îú‚îÄ Definir arquitectura
   ‚îú‚îÄ Compilar
   ‚îî‚îÄ Mostrar resumen

6. ENTRENAR (30-60 min en CPU) ‚è∞
   ‚îú‚îÄ 25 √©pocas (puede parar antes)
   ‚îú‚îÄ Balanceo de clases
   ‚îú‚îÄ Early stopping
   ‚îî‚îÄ Guardar mejor modelo

7. OPTIMIZAR UMBRAL (1-2 min)
   ‚îú‚îÄ Probar umbrales 0.1-0.9
   ‚îú‚îÄ Calcular F1 para cada uno
   ‚îî‚îÄ Retornar mejor umbral

8. EVALUAR (2-5 min)
   ‚îú‚îÄ Predicciones en test
   ‚îú‚îÄ Calcular m√©tricas
   ‚îú‚îÄ Generar 7 gr√°ficas
   ‚îî‚îÄ Mostrar resultados

9. GUARDAR (5-10 seg)
   ‚îú‚îÄ modelo_detector_smishing_mejorado.keras
   ‚îú‚îÄ umbral_optimo.npy
   ‚îî‚îÄ 7 gr√°ficas PNG
```

**Tiempo total: 40-90 minutos en CPU**

---

## Caracter√≠sticas Extra√≠das - Resumen

### Tabla Completa de las 23 Caracter√≠sticas

| # | Nombre | Tipo | Descripci√≥n | Importancia |
|---|--------|------|-------------|-------------|
| 1 | mensaje_longitud | Num√©rica | Longitud del mensaje | ‚≠ê‚≠ê |
| 2 | mensaje_palabras | Num√©rica | N√∫mero de palabras | ‚≠ê‚≠ê |
| 3 | mensaje_mayusculas_ratio | Ratio | Proporci√≥n de may√∫sculas | ‚≠ê‚≠ê |
| 4 | mensaje_caracteres_especiales | Ratio | Proporci√≥n de caracteres especiales | ‚≠ê‚≠ê |
| 5 | remitente_longitud | Num√©rica | Longitud del remitente | ‚≠ê |
| 6 | remitente_es_numerico | Binaria | ¬øEs n√∫mero? | ‚≠ê‚≠ê |
| 7 | remitente_tiene_letras | Binaria | ¬øTiene letras? | ‚≠ê |
| 8 | remitente_empieza_3 | Binaria | ¬øEmpieza por 3? | ‚≠ê‚≠ê‚≠ê |
| 9 | remitente_numero_corto | Binaria | ¬ø4-6 d√≠gitos? | ‚≠ê‚≠ê |
| 10 | remitente_movil_estandar | Binaria | ¬ø10 d√≠gitos con 3? | ‚≠ê‚≠ê‚≠ê |
| 11 | remitente_longitud_anormal | Binaria | ¬øLongitud extra√±a? | ‚≠ê‚≠ê |
| 12 | contiene_url | Binaria | ¬øTiene URL? | ‚≠ê‚≠ê‚≠ê |
| 13 | contiene_urgencia | Binaria | ¬øPalabras de urgencia? | ‚≠ê‚≠ê‚≠ê |
| 14 | contiene_dinero | Binaria | ¬øMenciona dinero? | ‚≠ê‚≠ê |
| 15 | contiene_banco | Binaria | ¬øMenciona banco? | ‚≠ê‚≠ê‚≠ê |
| 16 | contiene_verificacion | Binaria | ¬øPide verificar? | ‚≠ê‚≠ê‚≠ê |
| 17 | menciona_servicio_conocido | Binaria | ¬øServicio leg√≠timo? | ‚≠ê‚≠ê |
| 18 | tiene_errores_ortograficos | Binaria | ¬øErrores de ortograf√≠a? | ‚≠ê‚≠ê |
| 19 | sospecha_movil_fraudulento | Combinada | M√≥vil + se√±ales fraude | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 20 | contiene_premio | Binaria | ¬øMenciona premio? | ‚≠ê‚≠ê‚≠ê |
| 21 | monto_grande | Binaria | ¬øMonto >$100K? | ‚≠ê‚≠ê‚≠ê |
| 22 | llamada_accion_sospechosa | Binaria | ¬ø"Haz clic", etc? | ‚≠ê‚≠ê‚≠ê |
| 23 | patron_estafa_premio | Combinada | Premio + URL/acci√≥n | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## Arquitectura del Modelo - Detalles

### Par√°metros Totales: ~700K

```
Rama BERT:
  768 ‚Üí 512 ‚Üí 256
  Par√°metros: ~590K

Rama Num√©rica:
  23 ‚Üí 256 ‚Üí 128 ‚Üí 64
  Par√°metros: ~40K

Capas Combinadas:
  320 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 1
  Par√°metros: ~70K
```

### ¬øPor qu√© funciona?

1. **BERT captura sem√°ntica**: "Ganaste un premio" vs "Ganaste el partido"
2. **Caracter√≠sticas capturan patrones**: M√≥vil + URL = sospechoso
3. **Combinaci√≥n es poderosa**: Ambas fuentes de informaci√≥n
4. **Regularizaci√≥n previene overfitting**: L2 + Dropout + BatchNorm

---

## Ejemplo Completo de Predicci√≥n

### Entrada:
```
Mensaje: "Ganaste un premio de $5.000.000! Haz clic aqu√≠: bit.ly/premio123"
Remitente: "3209876543"
```

### Procesamiento:

**1. Caracter√≠sticas Num√©ricas (23):**
```
mensaje_longitud: 67
mensaje_palabras: 9
mensaje_mayusculas_ratio: 0.015
mensaje_caracteres_especiales: 0.134
remitente_longitud: 10
remitente_es_numerico: 1
remitente_tiene_letras: 0
remitente_empieza_3: 1          ‚≠ê
remitente_numero_corto: 0
remitente_movil_estandar: 1     ‚≠ê
remitente_longitud_anormal: 0
contiene_url: 1                 ‚≠ê
contiene_urgencia: 0
contiene_dinero: 1
contiene_banco: 0
contiene_verificacion: 0
menciona_servicio_conocido: 0
tiene_errores_ortograficos: 0
sospecha_movil_fraudulento: 1   ‚≠ê‚≠ê‚≠ê
contiene_premio: 1              ‚≠ê
monto_grande: 1                 ‚≠ê
llamada_accion_sospechosa: 1    ‚≠ê
patron_estafa_premio: 1         ‚≠ê‚≠ê‚≠ê
```

**2. BERT Embeddings (768):**
```
[0.234, -0.567, 0.891, ..., 0.123]  # Vector de 768 dimensiones
```

**3. Modelo:**
```
BERT (768) ‚Üí [512] ‚Üí [256] ‚îÄ‚îê
                             ‚îú‚îÄ‚Üí [320] ‚Üí [256] ‚Üí [128] ‚Üí [64] ‚Üí [1]
Nums (23)  ‚Üí [256] ‚Üí [64] ‚îÄ‚îÄ‚îò

Salida: 0.87 (87% probabilidad de fraude)
```

**4. Decisi√≥n:**
```
Umbral √≥ptimo: 0.30
0.87 > 0.30 ‚Üí üö® FRAUDULENTO
```

---

## Preguntas Frecuentes

### ¬øPor qu√© es tan lento?
- BERT procesa cada mensaje individualmente
- 110M par√°metros en BERT
- CPU es 10-20x m√°s lento que GPU

### ¬øPuedo usar solo las caracter√≠sticas sin BERT?
- S√≠, pero perder√≠as ~10-15% de accuracy
- BERT captura contexto que las caracter√≠sticas no pueden

### ¬øPor qu√© 23 caracter√≠sticas y no m√°s?
- Balance entre informaci√≥n y complejidad
- M√°s caracter√≠sticas ‚Üí m√°s riesgo de overfitting
- Estas 23 son las m√°s discriminativas

### ¬øC√≥mo s√© si el modelo funciona bien?
- Accuracy > 90%
- Recall (Fraudulento) > 95% (lo m√°s importante)
- F1-Score > 0.90
- Curva ROC cerca de la esquina superior izquierda

---

## Conclusi√≥n

El modelo combina:
- ‚úÖ **BERT**: Comprensi√≥n profunda del texto
- ‚úÖ **23 caracter√≠sticas**: Patrones espec√≠ficos de smishing
- ‚úÖ **Arquitectura dual**: Aprovecha ambas fuentes
- ‚úÖ **Regularizaci√≥n**: Previene overfitting
- ‚úÖ **Umbral optimizado**: Maximiza F1-score

**Resultado**: Detector robusto y preciso de smishing en espa√±ol.
